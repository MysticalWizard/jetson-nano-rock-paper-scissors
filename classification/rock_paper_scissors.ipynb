{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6p3137z9dH-"
      },
      "source": [
        "# Rock Paper Scissors Image Recognition Project\n",
        "### Interactive Classification Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX34nmRZ9dH_"
      },
      "source": [
        "### Camera\n",
        "First, create your camera and set it to `running`.  Uncomment the appropriate camera selection lines, depending on which type of camera you're using (USB or CSI). This cell may take several seconds to execute.\n",
        "\n",
        "<div style=\"border:2px solid black; background-color:#e3ffb3; font-size:12px; padding:8px; margin-top: auto;\">\n",
        "    <h4><i>Tip</i></h4>\n",
        "    <p>There can only be one instance of CSICamera or USBCamera at a time.  Before starting this notebook, make sure you have executed the final \"shutdown\" cell in any other notebooks you have run so that the camera is released. \n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD3lri1S9dIA",
        "outputId": "ef1025c8-c668-480d-80b0-2bd75dd07a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "crw-rw---- 1 root video 81, 0 Jul 24 05:26 /dev/video0\n"
          ]
        }
      ],
      "source": [
        "# Check device number\n",
        "!ls -ltrh /dev/video*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWFzV8Iu9dIB",
        "outputId": "ed4f4011-58fa-46f3-d547-3eca31a440f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "camera created\n"
          ]
        }
      ],
      "source": [
        "from jetcam.usb_camera import USBCamera\n",
        "from jetcam.csi_camera import CSICamera\n",
        "\n",
        "# for USB Camera (Logitech C270 webcam), uncomment the following line\n",
        "camera = USBCamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
        "\n",
        "# for CSI Camera (Raspberry Pi Camera Module V2), uncomment the following line\n",
        "# camera = CSICamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
        "\n",
        "camera.running = True\n",
        "print(\"camera created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9AwWK3a9dIC"
      },
      "source": [
        "### Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pcP4b9v9dID",
        "outputId": "9ae21b21-7f6b-49d5-e3ac-7677c4ebf66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fingers task with ['rock', 'paper', 'scissors'] categories defined\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from dataset import ImageClassificationDataset\n",
        "\n",
        "TASK = 'fingers'\n",
        "\n",
        "CATEGORIES = ['rock', 'paper', 'scissors']\n",
        "\n",
        "DATASETS = ['A']\n",
        "\n",
        "TRANSFORMS = transforms.Compose([\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "datasets = {}\n",
        "for name in DATASETS:\n",
        "    datasets[name] = ImageClassificationDataset('../data/classification/' + TASK + '_' + name, CATEGORIES, TRANSFORMS)\n",
        "    \n",
        "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvYXkYPy9dID"
      },
      "outputs": [],
      "source": [
        "# Set up the data directory location if not there already\n",
        "DATA_DIR = '/nvdli-nano/data/classification/'\n",
        "!mkdir -p {DATA_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zj9cCAr9dID"
      },
      "source": [
        "### Data Collection\n",
        "Execute the cell below to create the data collection tool widget. This cell should only take a few seconds to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1-EFhL09dIE",
        "outputId": "9fdaf0fa-8389-41d1-d42b-c46231e0e343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_collection_widget created\n"
          ]
        }
      ],
      "source": [
        "import ipywidgets\n",
        "import traitlets\n",
        "from IPython.display import display\n",
        "from jetcam.utils import bgr8_to_jpeg\n",
        "\n",
        "# initialize active dataset\n",
        "dataset = datasets[DATASETS[0]]\n",
        "\n",
        "# unobserve all callbacks from camera in case we are running this cell for second time\n",
        "camera.unobserve_all()\n",
        "\n",
        "# create image preview\n",
        "camera_widget = ipywidgets.Image()\n",
        "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
        "\n",
        "# create widgets\n",
        "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
        "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
        "count_widget = ipywidgets.IntText(description='count')\n",
        "save_widget = ipywidgets.Button(description='add')\n",
        "\n",
        "# manually update counts at initialization\n",
        "count_widget.value = dataset.get_count(category_widget.value)\n",
        "\n",
        "# sets the active dataset\n",
        "def set_dataset(change):\n",
        "    global dataset\n",
        "    dataset = datasets[change['new']]\n",
        "    count_widget.value = dataset.get_count(category_widget.value)\n",
        "dataset_widget.observe(set_dataset, names='value')\n",
        "\n",
        "# update counts when we select a new category\n",
        "def update_counts(change):\n",
        "    count_widget.value = dataset.get_count(change['new'])\n",
        "category_widget.observe(update_counts, names='value')\n",
        "\n",
        "# save image for category and update counts\n",
        "def save(c):\n",
        "    dataset.save_entry(camera.value, category_widget.value)\n",
        "    count_widget.value = dataset.get_count(category_widget.value)\n",
        "save_widget.on_click(save)\n",
        "\n",
        "data_collection_widget = ipywidgets.VBox([\n",
        "    ipywidgets.HBox([camera_widget]), dataset_widget, category_widget, count_widget, save_widget\n",
        "])\n",
        "\n",
        "# display(data_collection_widget)\n",
        "print(\"data_collection_widget created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEH4VXCT9dIE"
      },
      "source": [
        "### Model\n",
        "Execute the following cell to define the neural network and adjust the fully connected layer (`fc`) to match the outputs required for the project.  This cell may take several seconds to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v3Zr1vl9dIE",
        "outputId": "2747be25-9e75-40f9-f1ab-00f5f3b8457a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model configured and model_widget created\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# ALEXNET\n",
        "# model = torchvision.models.alexnet(pretrained=True)\n",
        "# model.classifier[-1] = torch.nn.Linear(4096, len(dataset.categories))\n",
        "\n",
        "# SQUEEZENET \n",
        "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
        "# model.classifier[1] = torch.nn.Conv2d(512, len(dataset.categories), kernel_size=1)\n",
        "# model.num_classes = len(dataset.categories)\n",
        "\n",
        "# RESNET 18\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
        "\n",
        "# RESNET 34\n",
        "# model = torchvision.models.resnet34(pretrained=True)\n",
        "# model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
        "    \n",
        "model = model.to(device)\n",
        "\n",
        "model_save_button = ipywidgets.Button(description='save model')\n",
        "model_load_button = ipywidgets.Button(description='load model')\n",
        "model_path_widget = ipywidgets.Text(description='model path', value='/nvdli-nano/data/classification/my_model.pth')\n",
        "\n",
        "def load_model(c):\n",
        "    model.load_state_dict(torch.load(model_path_widget.value))\n",
        "model_load_button.on_click(load_model)\n",
        "    \n",
        "def save_model(c):\n",
        "    torch.save(model.state_dict(), model_path_widget.value)\n",
        "model_save_button.on_click(save_model)\n",
        "\n",
        "model_widget = ipywidgets.VBox([\n",
        "    model_path_widget,\n",
        "    ipywidgets.HBox([model_load_button, model_save_button])\n",
        "])\n",
        "\n",
        "# display(model_widget)\n",
        "print(\"model configured and model_widget created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYjhNtn9dIF"
      },
      "source": [
        "### Live  Execution\n",
        "Execute the cell below to set up the live execution widget.  This cell should only take a few seconds to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2MiG35e9dIF",
        "outputId": "82f19069-40d2-4210-8ec3-2fce9f836d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "live_execution_widget created\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "from utils import preprocess\n",
        "import torch.nn.functional as F\n",
        "\n",
        "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
        "prediction_widget = ipywidgets.Text(description='prediction')\n",
        "score_widgets = []\n",
        "for category in dataset.categories:\n",
        "    score_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, description=category, orientation='vertical')\n",
        "    score_widgets.append(score_widget)\n",
        "\n",
        "def live(state_widget, model, camera, prediction_widget, score_widget):\n",
        "    global dataset\n",
        "    while state_widget.value == 'live':\n",
        "        image = camera.value\n",
        "        preprocessed = preprocess(image)\n",
        "        output = model(preprocessed)\n",
        "        output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
        "        category_index = output.argmax()\n",
        "        prediction_widget.value = dataset.categories[category_index]\n",
        "        for i, score in enumerate(list(output)):\n",
        "            score_widgets[i].value = score\n",
        "            \n",
        "def start_live(change):\n",
        "    if change['new'] == 'live':\n",
        "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget, score_widget))\n",
        "        execute_thread.start()\n",
        "\n",
        "state_widget.observe(start_live, names='value')\n",
        "\n",
        "live_execution_widget = ipywidgets.VBox([\n",
        "    ipywidgets.HBox(score_widgets),\n",
        "    prediction_widget,\n",
        "    state_widget\n",
        "])\n",
        "\n",
        "# display(live_execution_widget)\n",
        "print(\"live_execution_widget created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDlQTU5J9dIF"
      },
      "source": [
        "### Training and Evaluation\n",
        "Execute the following cell to define the trainer, and the widget to control it. This cell may take several seconds to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS5mFDj39dIG",
        "outputId": "aabf6a9e-a1eb-4892-fbbb-ec9fce393ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainer configured and train_eval_widget created\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
        "eval_button = ipywidgets.Button(description='evaluate')\n",
        "train_button = ipywidgets.Button(description='train')\n",
        "loss_widget = ipywidgets.FloatText(description='loss')\n",
        "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
        "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
        "\n",
        "def train_eval(is_training):\n",
        "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
        "    \n",
        "    try:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        state_widget.value = 'stop'\n",
        "        train_button.disabled = True\n",
        "        eval_button.disabled = True\n",
        "        time.sleep(1)\n",
        "\n",
        "        if is_training:\n",
        "            model = model.train()\n",
        "        else:\n",
        "            model = model.eval()\n",
        "        while epochs_widget.value > 0:\n",
        "            i = 0\n",
        "            sum_loss = 0.0\n",
        "            error_count = 0.0\n",
        "            for images, labels in iter(train_loader):\n",
        "                # send data to device\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                if is_training:\n",
        "                    # zero gradients of parameters\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # execute model to get outputs\n",
        "                outputs = model(images)\n",
        "\n",
        "                # compute loss\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                if is_training:\n",
        "                    # run backpropogation to accumulate gradients\n",
        "                    loss.backward()\n",
        "\n",
        "                    # step optimizer to adjust parameters\n",
        "                    optimizer.step()\n",
        "\n",
        "                # increment progress\n",
        "                error_count += len(torch.nonzero(outputs.argmax(1) - labels).flatten())\n",
        "                count = len(labels.flatten())\n",
        "                i += count\n",
        "                sum_loss += float(loss)\n",
        "                progress_widget.value = i / len(dataset)\n",
        "                loss_widget.value = sum_loss / i\n",
        "                accuracy_widget.value = 1.0 - error_count / i\n",
        "                \n",
        "            if is_training:\n",
        "                epochs_widget.value = epochs_widget.value - 1\n",
        "            else:\n",
        "                break\n",
        "    except e:\n",
        "        pass\n",
        "    model = model.eval()\n",
        "\n",
        "    train_button.disabled = False\n",
        "    eval_button.disabled = False\n",
        "    state_widget.value = 'live'\n",
        "    \n",
        "train_button.on_click(lambda c: train_eval(is_training=True))\n",
        "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
        "    \n",
        "train_eval_widget = ipywidgets.VBox([\n",
        "    epochs_widget,\n",
        "    progress_widget,\n",
        "    loss_widget,\n",
        "    accuracy_widget,\n",
        "    ipywidgets.HBox([train_button, eval_button])\n",
        "])\n",
        "\n",
        "# display(train_eval_widget)\n",
        "print(\"trainer configured and train_eval_widget created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHz7jLbc9dIG"
      },
      "source": [
        "### Display the Interactive Tool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7Ej1PI9dIG"
      },
      "source": [
        "The interactive tool includes widgets for data collection, training, and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxK5WkUU9dIG"
      },
      "source": [
        "Execute the cell below to create and display the full interactive widget.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d4bed84224804156a53a027a48459d64"
          ]
        },
        "id": "It_5Tr2K9dIH",
        "outputId": "dd3b9b71-d88f-4c2e-8749-af1a56a94a69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4bed84224804156a53a027a48459d64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Combine all the widgets into one display\n",
        "all_widget = ipywidgets.VBox([\n",
        "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
        "    train_eval_widget,\n",
        "    model_widget\n",
        "])\n",
        "\n",
        "display(all_widget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qJ2rmtX9dIH"
      },
      "source": [
        "<h1 style=\"background-color:#76b900;\"></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOhy5Xfi9dIH"
      },
      "source": [
        "## Shut down the camera and/or notebook kernel to release the camera resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dt2cX0_9dIH"
      },
      "outputs": [],
      "source": [
        "# Attention!  Execute this cell before moving to another notebook\n",
        "# The USB camera application only requires that the notebook be reset\n",
        "# The CSI camera application requires that the 'camera' object be specifically released\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "\n",
        "if type(camera) is CSICamera:\n",
        "    print(\"Ignore 'Exception in thread' tracebacks\\n\")\n",
        "    camera.cap.release()\n",
        "\n",
        "os._exit(00)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "rock_paper_scissors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}